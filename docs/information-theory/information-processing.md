---
title: 信息论:信息处理
comments: true
---

???+ info "信息"

    本节课是信息动力学的第一节课, 具体目标包括:

    - 研究信息动力学的哲学背景: 需要了解信息动力学如何解释复杂系统中的信息流动和处理. 信息是如何生成, 传递, 消化和消散的, 通过信息动力学来了解系统的行为和变化
    - 理解熵率: 熵率是信息论中度量信息生成速率的重要指标, 反映了系统的随机性和不确定性. 通过研究熵率, 可以了解系统的动态行为, 尤其是长期行为的复杂性和不可预测性

## 背景

考虑一个问题, 我们能否将生物计算适用到主流的冯诺依曼架构的计算机科学计算范式中?

Mitchell提出了如下对比:

|问题|计算机科学|生物计算|
|-|-|-|
|信息的角色是什么 |数字化静态存储介质|模拟状态, 空间和时间上分布的模式, 通过统计采样收集信息|
|信息如何传递处理|确定性, 串行, 零误差, 集中的规则|去中心化, 并行, 本地, 细粒度的随机交互, 利用随机性|
|信息如何获得功能和意义|由人类设计|自然选择|

它还指出, "language of dynamical systems may be more useful than language of computation", 即"动态系统的语言可能比计算语言更有用". 这意味着在理解复杂系统的时候, 传统的计算范式可能不足以解释解释系统中的信息流动. 相反, 动态系统理论, 一种通过研究系统随时间变化的行为来解释信息处理, 可能更加适合复杂系统的本质.

<figure markdown='1'>
![](https://img.ricolxwz.io/bcbe3e386fa8b1a7ed14771b66760ddc.png){ loading=lazy width='400' }
</figure>

图中展示了一个系统(🧠)如何通过动态过程, 将初始状态(左脑, 带有输入)通过某种动态变化, 转变为最终状态(右脑, 带有输出). 输入通过这个动态过程被转换, 最终得到输出. 

它的观点强调, 内在的信息处理发生在系统经历动态过程时, 即当系统通过接受输入并转变为初始状态为某种晚期状态时. 这意味着系统的信息处理并不是通过离散的步骤或者预定的规则来完成的, 而是通过连续的, 动态的状态变化来实现的.

生物计算或者类生物信息计算有如下特点:

- 分布式, distributed, 与图灵机不同, 生物信息计算不是集中在某个地方的, 而是分布式的, 这意味着信息在整个系统中分布和处理, 可能没有单一的控制中心

    ???+ note "笔记"

        所谓的分布式计算, 是指任何系统中多个部分一起处理信息的方式:

        - 大脑处理信息是通过多个神经元一起工作
        - 鸟群集体决定飞行方向
        - 蚂蚁寻找食物通过相互交流找到最佳路线
        - 甚至可以说, 整个宇宙通过自然法则在"计算"它的未来

- 持续性, ongoing, 与图灵机的确定性程序不同, 生物信息计算是持续的, 内在计算, intrinsic computation, 或者信息处理并不一定有明确的结束点. 生物系统的信息计算可能是不断进行的, 不像计算机程序那样可以明确完成任务并结束

我们可以从一下的几个方面描述一个生物系统的信息处理:

- 信息存储: 系统如何存储信息
- 信息传递: 系统中的信息如何从一个地方传递到另一个地方
- 信息修改: 系统如何对存储和传递的信息进行修改和操作

在传统的计算机系统中, 这些信息处理操作的元素(例如存储器, 处理器, 总线等)是清晰可见的, 便于识别. 然而, 在生物计算中, 这些信息处理过程往往更加复杂和隐含, 不易清晰地识别出来.

即, 通过使用系统本身的语言来量化其内在的计算过程, 我们可以理解自然界是如何"计算"的, 并且了解这些计算过程为何如此复杂. 

???+ note "笔记"

    什么是"系统本身的语言"? 系统本身的语言指的是我们用来描述和理解一个特定系统的规则和模式. 例如:

    - 大脑的语言: 在大脑中, 神经元通过电信号和化学信号进行信息传递, 因此我们需要使用神经科学的术语, 如突触, 神经元, 神经网络等来描述大脑的信息处理方式
    - 基因调控网络的语言: 在基因调控网络中, 基因表达收到各种调节因子的控制, 因此基因表达, 蛋白质调控, 转录因子等就是系统的语言用来描述基因如何影响细胞行为
    - 蚁群的语言: 蚂蚁通过信息素进行沟通和决策, 因此, 信息素浓度, 路径选择, 分布式决策等术语就是描述蚁群行为的语言

## 信息动力学

信息动力学研究的是如何通过存储, 传递和修改来研究复杂系统的下一个状态, 即$x_{n+1}$.

<figure markdown='1'>
![](https://img.ricolxwz.io/2d81d2714fc7e8dfc51484daa6b037b9.png){ loading=lazy width='400' }
</figure>

图中展示了一个复杂系统, 由多个节点($Y_1, A, X, Y_2, B$)组成. 这些节点相互作用, 影响着系统的演变, 箭头表示信息或者影响的传递路径. 下方是时间序列示意图, 展示了每个时刻变量的状态(用$0$和$1$表示). 时间是自上而下推进的, 表示状态随时间的演化. 图中系统的绿色竖条代表的是$X$的时间序列, 显示了从$n-k+1$到$n+1$的所有信息.

从图中, 我们产生了一些疑问:

- 信息从哪里来, 如何测量? $x_{n+1}$的信息来源是什么, 如何测量变量状态的变化? 我们应该在哪些节点(如$Y_1, A$等)或交互中寻找信息的来源?
- 如何建模信息处理? 主要关注系统如何存储先前的信息, 以及信息在节点之间是如何传递的? 信息的传递如何归因与特定的路径或者节点?
- 能否对信息进行划分? 如何判断它们之间是否有重叠?

前面我们提到了信息动态和时间序列的分析方式, 这些不仅仅是理论, 而且可以实际应用在像是"剪刀-石头-布"这类的游戏和元胞自动机这种系统中.

### 时间序列

有数个时间序列过程$\bm{X}, \bm{Y}, \bm{Z}$, 可以代表不同的随机想想在不同时间点的表现, 例如: $\bm{X}$可以表示某个地区的每日温度, $\bm{Y}$可以表示同一时刻下该地区的湿度, $\bm{Z}$可以表示某种市场指标如股价指数的变化. 每个过程如$X$由随机变量$\{..., X_{n-1}, X_{n}, X_{n+1}, ...\}$组成, 这些随机变量按照时间$n$进行计数. 而它们的实际值可以用$\{..., x_{n-1}, x_n, x_{n+1}\}$表示.

### 块向量 {#块向量}

块向量$\bm{X}_{n^{(k)}}$表示从$n-k$到$n$的一系列连续随机变量, 即$\bm{X}^{(k)}=\{X_{n-k+1}, ..., X_n\}$. 而它的实际值用$x_n^{(k)}=\{x_{n-k+1}, ..., x_n\}$表示.

---

这里, 我们产生了一个问题, 随机变量$X_{n+1}$中的信息是从哪里来的? 它是如何从其他变量$Y_m, Z_m, ...$中获得的. 尤其对于$m\leq n$的情况.

### 块熵 {#块熵}

块熵, block entropy是用来衡量一段固定长度的序列中包含的信息量或者不确定性. 它扩展了单步熵的概念, 后者只是衡量单个状态的熵.

???+ example "例子"

    假设我们在观察一个二进制序列, 如果这个序列完全随机, 那么没增加一个$1$或者$0$, 整个序列的块熵就会增长, 因为每个新状态都是不确定的, 包含新的信息. 但是, 如果序列是完全确定的, 或者有规律的, 那么块熵就不会增长, 因为你已经知道接下来是什么, 没什么新信息可以获得.

块熵$H(X_1, X_2, ..., X_n)$是前$n$个状态的联合熵, 它表示这$n$个状态共同带来的总信息量. $H(X_1, X_2, ..., X_n)=-\sum_{x_1, x_2, ..., x_n}p(x_1, x_2, ..., x_n)\log p(x_1, x_2, ..., x_n)$.

### 熵率

熵率可以度量当块的长度增加的时候, [块熵](#块熵)对于单位长度的增长速率. $H'_{\mu \bm{X}}=\lim_{n\rightarrow \infty}\frac{1}{n}H(X_1, X_2, ..., X_n)$. 这表示随着块长度$n$的增大, 单位长度上带来的平均不确定性. 或者也可以用[块向量](#块向量)来表示$H'_{\mu \bm{X}}=\lim_{n\rightarrow \infty}\frac{1}{n}H(X_n^{(n)})$, 更加简洁.

???+ tip "Tip"

    在时间序列中, 你可以将其理解为每一个时间步的所带来的平均熵. 举一个例子, 熵表示的是整个天气时间序列中天气状况的不确定性, 例如前三天的天气联合熵$H$可能是$H(X_1, X_2, X_3)=1.5$. 熵率$H'$表示平均每天(每个时间步)天气的平均不确定性, 即$\frac{1.5}{3} = 0.5$, 表示平均每天带来了$0.5$比特的信息量.

熵率不仅度量块长度变化的熵增量, 也可以度量下一个随机变量在给定之前变量的情况下的熵增量. $H_{\mu \bm{X}}=\lim_{n\rightarrow \infty}\frac{1}{n}H(X_n|X_1, X_2, ..., X_{n-1})$. 同样也可以用[块向量](#块向量)来表示$H_{\mu \bm{X}}=\lim_{n\rightarrow \infty}H(X_n|X_{n-1}^{(n-1)})$.

<figure markdown='1'>
![](https://img.ricolxwz.io/1ff61a6c75d9e4df8e1b1eb4dc22f575.png){ loading=lazy width='200' }
</figure>

对于平稳增长来说, $H'_{\mu \bm{X}}=H_{\mu X}$.