---
title: 信息论:估计器
comments: true
---

估计器, Estimator是一种用于从观测数据中推断未知参数或者数量的统计工具. 在机器学习中, "估计器"一词广泛用于指代模型和算法, 像线性回归, 支持向量机, 神经网络等模型都是不同类型的估计器. 估计器是一个更广义的词语, 它可以指代用于分类, 回归或者其他类型的模型; 而分类器是一种估计器的特殊形式, 专门用于分类任务, 输出的是离散的标签. 估计器的一些可能的特征包括: 

- 无偏性: 若估计器的期望等于真实参数值, 则称估计器是无偏的. 即当对一个无偏估计器进行反复独立采样的时候, 长期来看, 得到的估计值的平均值应该等于真实参数
- 一致性: 若随着样本量的增加, 估计值收敛到真实参数值, 则称估计器是一致的
- 效率: 在无偏估计器中, 方差最小的估计器被认为是最有效的
- 充分性: 若估计器中包含了样本中关于未知参数的所有信息, 则称其为充分统计量

???+ warning "注意"

    无偏性不能保证一致性: 一个无偏估计器的期望值等于真实参数值, 但是这并不能保证每次采样的估计值都等于真实值. 假设我们有一个无偏估计器, 其期望值等于真实值, 但是它的方差不随着采样的样本量增加而减少, 那么即使在较大样本量的情况下, 估计器的估计值可能仍然会和真实值有很大偏差, 因此, 这种估计器是无偏的, 但是它并不具有一致性. 如果一个估计器是一致的, 意味着当样本量增加的时候, 估计值不仅期望需要接近真实值, 即需要接近于是一个无偏估计器, 还需要其方差收敛到$0$. 

## 最大似然估计器

最大似然估计器, Maximum Likelihood Estimator, MLE是一种通过最大似然估计, Maximum Likelihood Extimation, MLE找到模型参数的估计器. 它的核心思想是:

1. 给定一组观测数据, 我们假设数据来自某个概率分布(例如正态分布, 泊松分布等)
2. 设定模型的参数$\theta$, 通过定义似然函数来衡量这些参数下观测数据出现的可能性
3. 最大化似然函数: 即找到能够最佳模拟/贴合观测数据(观测数据出现可能性最大)的参数$\theta$, 这些参数再代入到我们假设的分布中, 得到的就是最大似然估计器

最大似然估计器在机器学习中有很多应用. 逻辑回归分类器就是一种典型的最大似然估计器. 在逻辑回归中, 我们选择的对数似然函数是$\log p(D)=\sum_{i=1}^n(y_i\log\sigma(\bm{w}^T\bm{x_i})+(1-y_i)\log[1-\sigma(\bm{w}^T\bm{x_i})])$, 我们通过最大化这个对数似然函数, 找到了能够最佳描述观测数据(即训练集样本标签)的参数$\bm{w}$. 

### 离散和连续

对于离散数据的情况, 最大似然估计的推导过程非常直观, 当我们通过观测数据计算出每个符号出现的频率的时候, 这些频率就是使得观测数据出现概率最大的参数. 这是因为在离散的情况下, 我们已经知道了数据的分布, 对比连续, 我们还要取拟合正态分布曲线, 高斯分布曲线... 

如假设你掷色子$100$次, 结果是: $1$出现$15$次, $2$出现$20$次, $3$出现$10$次, $4$出现$25$次, $5$出现$15$次, $6$出现$15$次. 在这种情况下, 我们已经知道了概率分布, 无须进一步拟合复杂的分布. 直接计算其频率即可. 

假设你测量某个物体的长度, 得到了一组连续的值, 比如$X_1=10.2$, $X_2=9.8$, $X_3=10.0$等等, 你假设这些数据服从正态分布, 在这种情况下, 你需要通过最大似然估计来拟合这组数据, 找到正态分布的两个参数$\mu$和$\sigma$, 使得该分布能够最大的拟合观测数据.

其实, 在离散的情况下, 最大化/拟合这个似然函数的过程就是我们计算每个符号频率的过程, 当我们计算出所有符号的频率的时候, 其结果就是似然函数最大了.

## 方差和偏差

当我们计算从经验值计算信息论的测量值如熵的时候, 我们计算的其实是真实值$H$的估计$\hat{H}$.

如投硬币, 我们投$6$次是正面, $4$次是反面, 计算得到的估计熵是$-\frac{2}{5}\log_2\frac{2}{5}-\frac{3}{5}\log_2\frac{3}{5}$. 但是真实值是$-\frac{1}{2}\log_2\frac{1}{2}-\frac{1}{2}\log_2\frac{1}{2}$. 在这里, 我们的估计器是最大似然估计器. 当我们进行多次这样的独立的投硬币实验之后(每组10次), 我们就会得到一个估计熵的平均值, 即$<\hat{H}>$, 然后, 我们就可以计算偏差, 即$B=<\hat{H}>-H$. 还可以得到方差$v(\hat{H})$.

在数据量有限的时候, 估计器的偏差和方差会更加显著. 如熵在有限数据下往往会被低估, 即观测到的熵值通常比真实的熵值要小; 互信息往往会被高估, 即观测到的互信息通常比真实值要大.

当我们计算离散随机变量的熵的时候, 如果样本量$N$不够大, 直接使用频率估计的熵可能会偏低, 通过下列公式, 我们可以对熵的估计值进行修正, 补偿因为有限样本引入的偏差, 从而得到更接近真实值随机变量的高斯分布

## 分箱 {#分箱}

在很多情况下, 我们面对的是连续数据, 而我们希望通过离散的统计方法进行处理, 为了做到这一点, 我们可以对连续数据进行离散化, 也就是通过分箱, binning的方式将连续的数据分成多个离散的区间或者类别. 这其实是机器学习预处理的一种. 如图所示.

<figure markdown='1'>
![](https://img.ricolxwz.io/d497a05e2096fd85dfc4dcf04ffffd62.png){ loading=lazy width='400' }
</figure>

在图中展示的是在二维平面中, 对连续数据进行分箱. 每个箱📦代表的是某个$x$和$y$范围内的数据, 在同一个箱中的数据点被视为同类数据, 例如, 在图中这个红色的箱有$2$个数据点, 因此$2/18$用来表示该箱的频率. 计算箱的频率的过程就是最大似然估计器来拟合这组观测数据(即计算它的频率)的过程.

### 方式

一般来说, 有多种方式进行分箱, 其中的两种分箱方式为:

- 等大小分箱: 所有箱的大小相同, 这是最简单的方式, 但有时会忽略数据中的细微结构
- 最大熵分箱: 分箱后使数据分布的熵最大. 如果数据在某些特定区域有密集分布, 而其他区域较少, 采用等宽分箱可能会导致部分箱中的数据点的数量非常小, 熵也较小, 最大分箱可以在这种情况下使数据更加均匀地分配到每个箱中, 使分箱后的离散数据包含的信息更多

### 粒度

分箱的粒度(bin size)会对结果有显著影响:

- 粒度过大: 如果箱的数量过少, 可能会掩盖数据中重要细节
- 粒度过小: 如果箱的数量过多, 可能会导致过拟合, 数据的噪声会显著影响最终的结果

## 微分熵

刚才我们讲到了可以利用分箱的方式将连续数据转为离散数据然后计算它们的熵, 那么有没有一种方式可以直接计算连续数据的熵呢? 是有的, 那就是微分熵. 微分熵的公式类似于离散熵的定义, 但是它是对于概率密度函数$f(x)$进行积分, 而不是求和. 其公式为$H_D(X)=-\int_{S_X}f(x)\log f(x)dx$, 其中$x$是连续型随机变量, $f(x)$是概率密度函数, $f(x)>0$.

$H_D(X)$有一些有趣的性质:

- $H_D(X+a)=H_D(X)$
- $H_D(aX)=H_D(X)+\log a$
- $H_D(X)$可以是负数
- $H_D(X)=\lim_{\Delta\rightarrow 0}H^{\Delta(X)}+\log\Delta$, 这里的$\Delta$是离散化后箱的大小

还可以通过对微分熵的加减形成其他信息量, 如微分互信息$I_D(X;Y)$. 微分互信息继承了[离散互信息的性质](/information-theory/what-is-information/#互信息), 如$I_D(X;Y)>0$, 还有一些额外的性质: 

- $I_D(aX;bY)=I_D(X;Y)$
- $I_D(X;Y)=\lim_{\Delta\rightarrow 0}I^{\Delta}(X;Y)$, 这里的$\Delta$是离散化后箱的大小 

可以看出, 计算熵的过程中, 最重要的就是要知道PDF函数是什么, 这个时候就要用到最大似然估计器, 找到最优的参数以描述观测数据.

在JIDT中, 有三种估计方法可供选择: 

- 高斯模型
- 核模型
- 最临近模型

## 协方差[^1]

假设有两个随机变量$X$和$Y$, 它们之间的协方差是$Cov(X,Y)=E[(X-E(X))(Y-E(Y))]$. 公式简单的翻译就是, 每个时刻的$X$与其均值$E(X)$之差乘以每个时刻的$Y$与其均值$E(Y)$之差, 再对每个时刻的乘积求和之后求一个均值.

这可以通俗的理解为: 两个变量在变化的过程中是同方向变化? 还是反方向变化? 同向或反向程度如何?

- 你变大, 同时我也变大, 说明两个变量是同向变化的, 这个时候协方差就是正的
- 你变大, 同时我变小, 说明两个变量是反向变化的, 这个时候协方差就是负的

从数值上来看, 协方差的数值越大, 两个变量的同向程度也就越大, 反之亦然. 如果接近$0$, 说明$X$和$Y$之间的运动是不规律的, 很有可能某个时刻$X-E(X)$的值和$Y-E(Y)$的乘积为正, 另外一个时刻的乘积为负.

### 协方差矩阵 

协方差矩阵用于衡量多个随机变量的协方差, 对于一个包含$d$个随机变量$X_1, X_2, ..., X_d$的向量 $\bm{X}=(X_1, X_2, ..., X_d)^T$, 协方差矩阵是一个$d\times d$的对称矩阵, 每个元素$Cov(X_i, X_j)$表示第$i$个元素和第$j$个随机变量的协方差.

## 相关系数 {#相关系数}[^1]

对于相关系数, 它和协方差很像, 公式为$\rho=\frac{Cov(X,Y)}{\sigma_X\sigma_Y}$. 相关系数可以被看为一种特殊的协方差: 剔除了两个变量量纲的影响, 标准化之后的协方差.

很多时候, $X$和$Y$虽然是同向变化, 但是它们变化的幅度不一样, 如[图](https://img.ricolxwz.io/ef6d91e949fa6691ef2eae243a51a766.png)所示, 情况一的协方差会远远大于情况二的协方差.

于是, 为了能准确研究两个变量在变化过程中的相似程度, 我们要把变化幅度对协方差的影响, 从协方差里面剔除掉, 于是就有了相关系数的公式.

标准差是方差开方之后得到的, $\sigma_X=\sqrt{E((X-E(X)^2)}$. 越偏离平均值其方差/标准差就越大.

由于标准差的作用, 相关系数总是在$1$和$-1$之间.

## 高斯模型

???+ info "信息"

    高斯分布就是正态分布. 下面全部都用高斯分布代替.

<figure markdown='1'>
![](https://img.ricolxwz.io/6d1ea415085ecd372aa60f6bdbf63fae.png){ loading=lazy width='800' }
</figure>

使用高斯模型的前提是数据或转换后的数据要近似符合高斯分布. 可以通过Shapiro-Wilk检验或者Kolmogorov-Smirnov检验数据是否近似符合高斯分布. 若数据近似符合高斯分布, 我们可以使用最大似然估计法找到高斯分布的参数, 即每一个随机变量的期望和标准差, 进而可以计算协方差矩阵$\Omega_{\bm{X}}=\bm{X}\bm{X}^T$.

假设一个随机变量$X$符合高斯分布, 那么它的微分熵为$H(\bm{X}=\frac{1}{2}\ln[(2\pi e)^d|\Omega_{\bm{X}}|])$.

同样的, 还可以通过对微分熵的加减形成其他信息量, 如微分互信息.

对于单个符合高斯分布的随机变量$X$, 其标准差为$\sigma$, 其熵的计算公式为$H(X)=\ln((2\pi e)^{1/2}\sigma)$.; 

对于两个符合高斯分布的随机变量$X$和$Y$, 两者之间的[相关系数](#相关系数)为$\rho$, 其互信息的计算公式为$I(X;Y)=-\frac{1}{2}\ln(1-\rho^2)$.

要注意的是, 使用上述计算公式的前提是在模型中, 我们假设随机变量之间是通过线性关系进行耦合的, 也就是说假设它们之间的影响是线性的. 这个是协方差用于熵计算的基本假设, 因为协方差描述的是两个随机变量之间的线性相关性. 若两个变量在线性上无关, 但是存在非常复杂的非线性关系, 如二次, 指数等, **协方差就无法反映非线性关系中两个随机变量的相关性**, 也就无法表示真实的熵和互信息等信息量. 说白了, 按照上面公式算出来的信息量度量值只考虑了在线性上的关系, 没有考虑非线性.

优缺点:

- 优点: 很快($O(Nd^2)$), 无需考虑其他的参数, 只需要考虑期望和标准差
- 缺点: $H(\bm{X})$反映的是线性关系的不确定性

## 核密度估计

机器学习中的[非线性支持向量机](https://ml.ricolxwz.de/algorithm/svm/#non-linear-svm)用到了核函数, 我们通过一个变换$\phi$将原始特征空间映射到新的特征空间, 在新的特征空间中, 决策边界可能会变成线性的. 其中我们使用[核函数](https://ml.ricolxwz.de/algorithm/svm/#kernel-function)在原始特征空间计算点积, 从而减少了算力消耗.  

注意的点来了, 这个小节里面的核函数和机器学习里面的核函数是**不一样的**. 机器学习里面的核函数是核方法的核函数, 但是这里用的是核密度估计(KDE)中的核函数.

- 核密度估计中的核函数: 主要用于定义一个权重函数, 在一定区域内给数据点分配权重, 以估计局部密度或者平滑数据, 并不计算点积
- 核方法的核函数: 主要用于计算点积, 以便在隐式高维特征空间中衡量样本点的相似性

???+ quote "教授回复"

    A kernel function is simply a function which we apply to each data point, centring the function on it and effectively summing the value of that function at surrounding data points, to compute some implicit feature of the data point and how it relates to nearby points (e.g. usually a probability estimate for that point or a computation of how similar it is to other points in the data set). The box kernel is simply a function which is constant in a radius (or box) around the focus point, and when applying it we just then count other data points which fall within this radius (or box). This is useful for probability density estimation. There are of course more complex kernel functions that we can apply in order to approximate other quantities, e.g. RBF and polynomial functions, which are useful in machine learning.

这里, 我们用到的是矩形核函数, 如果两个点在原始空间中的距离不超过$r$, 那么它们在新的特征空间中被归为同一个矩形, 值为$0$, 如果两个点的距离超过$r$, 那么它们在特征空间中被归为不同矩形, 值为$1$. 我们可以利用这个矩形核函数计算映射到新的特征空间后, 样本$(x_n, y_n)$的周围的矩形框(宽度为$r$)内的点在所有点中的占比, 首先计算没有归一化的密度: $\hat{p}(x_n, y_n)=\frac{1}{N}\sum_{n'=1}^N\Theta(\left|\begin{smallmatrix}
x_n - x_{n'} \\
y_n - y_{n'}
\end{smallmatrix}\right|-r)$, $\left|\begin{smallmatrix}
x_n - x_{n'} \\
y_n - y_{n'}
\end{smallmatrix}\right|-r$表示的是样本点$(x_n, y_n)$和另一个样本点$(x'_n, y'_n)的距离$. 这个结果需要经过归一化, $\hat{f}(x_n, y_n)=\frac{\hat{p}(x_n, y_n)}{2r}$, 因为它衡量的是$(x_n, y_n)$所属的宽度为$r$的矩形内的点的数量在所有点中的占比, 然后将这个概率近似看作是$(x_n, y_n)$的概率密度, 或者说衡量的是其他点落在以$(x_n, y_n)$所在的宽度为$r$的矩形区域里的概率.

<figure markdown='1'>
![](https://img.ricolxwz.io/705268d17c4514bc1de00244c0c0d1a4.png){ loading=lazy width='300' }
</figure>

熵可以计算为$H(X)=\frac{1}{N}\sum_{n'=1}^N\log \hat{f}(x_n)$. 互信息的问题就转化为如果我们知道某一个点落在宽度为$r$的矩形区域内对另一个点落在宽度为$r$的矩形区域内的影响.

优缺点:

- 优点: 无模型, 不依赖任何特定的概率分布假设, 如高斯模型, 它假设数据符合高斯分布, 它是一种非参数方法, 可以适用于多种不同的分布
- 缺点: 对$r$的取值很敏感

### KSG {#ksg}

矩形核函数使用一个固定大小的矩形区域来计算样本点周围的密度, 如果其他点落在这个区域内, 给他赋予权重$1$, 超出则赋予权重$0$. 

KSG, Kraskov-Stogbauer-Grassberger, 是一种与矩形核函数类似的估计概率密度的方法, KSG使用KNN算法来估计样本点周围的局部密度, 对于每个样本点, 它找到最近的$k$个邻居, 并根据这些邻居的分布情况来估计概率密度.

<figure markdown='1'>
![](https://img.ricolxwz.io/86a4b3bfa5544f950d82cfa4292110fe.png){ loading=lazy width='350' }
</figure>

KSG相较于矩形核密度有更低的偏差, 其一是因为可以利用Kozachenko-Leonenko熵估计器对偏差进行修正, 其二是因为在联合空间中使用固定数量的$k$个邻居的最邻近计数能够更好的抵消偏差.

KSG有两种算法, 算法1的互信息计算公式为$I^{(1)}(X;Y)=\psi(K)-<\psi(n_x+1)+\psi(n_y+1)>+\psi(N)$.

- 算法1的方差小, 但是偏差大
- 算法2的方差大, 但是偏差小

优缺点:

- 优点: 无模型, 不依赖任何特定的概率分布假设, 如高斯模型, 它假设数据符合高斯分布, 它是一种非参数方法, 可以适用于多种不同的分布. 通过基于最邻近的密度估计方法, KSG对传统互信息估计中的系统性偏差进行了修正, 保证估计结果更加接近真实值. 能够以较少的数据给出准确的互信息估计结果. 它对参数$K$非常稳定, 意味着我们不需要精准调节这个参数, 可以认为KSG是参数无关的
- 缺点: KSG依赖最邻近搜索, 它的计算复杂度较高, 特别是在高维数据上, 寻找最邻近点需要耗费较多的计算资源, 因此时间效率较低; 虽然KSG的事件复杂度较高, 但是可以利用快速的最邻近搜索算法, 如KD树等数据结构, 将时间复杂度降低为$O(KN\log N)$

[^1]: 如何通俗易懂地解释「协方差」与「相关系数」的概念？ - 知乎. (n.d.). Retrieved September 8, 2024, from https://www.zhihu.com/question/20852004

