---
title: 信息论:信息存储
comments: false
---

## 背景 

试着考虑如下的问题: 我们需要从历史的变量中知晓多少数据才能帮助我们做出准确的预测. 在建立时间序列等动态模型的时候, 模型中需要存储多少的过去的信息, 以便正确描述该变量未来的动态?

对于人来说, 我们可以用肉眼或者说是直觉根据过去的值判断下一个值.

<figure markdown='1'>
![](https://img.ricolxwz.io/6a17f69ad6bd7041781666e8eafdb028.png){ loading=lazy width='500' }
</figure>

如上面的图片, 需要根据已经给出的数据来预测$x(n)$的下一个值. 在这个过程中, 做了什么假设呢? 例如, 我们可能假设这些值在后续的$n$中继续保持不变, 或者呈现某种趋势/规律. 具体是从哪里获取信息来做出这个假设的呢? 例如, 我们可能会说基于所有的数据点保持不变这一信息, 做出了下一个值也不变的假设. 那么这个从过去获取信息的量是多少呢? 

## 主动信息存储

主动信息存储, Active Information Storage, 探讨的是变量的过去状态中平均存储了多少信息可以用于预测下一次观测值.

整个过去状态对未来的贡献可以被表示为$A_X = \lim_{k\rightarrow \infty}I(\bm{X}_n^{(k)};X_{n+1})$, 其中$\bm{X}_n^{(k)}=\{X_{n-k+1}, ..., X_{n-1}, X_n\}$, 表示过去的$k$个状态, $k$趋近于无穷大表示我们考虑的是整个过去的状态, 最终得到的互信息表示的就是整个过去状态对未来的贡献. 或者也可以写为$k$个过去状态对未来的贡献, 这个时候$k$就不是趋近于无穷了, 是一个值, $A_X(k)=I(\bm{X}_n^{(k)};X_{n+1})$. 

$A_X$还可以表示为$A_X=H(X_{n+1})-H_{\mu X}$, 其中, $H_{\mu X}$是[熵率](/information-theory/information-processing/#entropy-rate), 表示的是每个时间步所带来的平均熵, 或者说是单位块长度的熵增量; $H(X_{n+1})$表示的是对未来状态的全部不确定性. 这个公式可以理解为去掉预测的不确定性后, 所带来的信息增益.

$A_X(k)$可以表示在有限的历史长度$k$下, 当前状态对未来状态的预测能力的平均值, 还可以被表示为$A_X(k)=<\log_2 (\frac{p(x_{n+1}|x_n^{(k)})}{p(x_{n+1})})>$, 其中$a_X(k)=\log_2 (\frac{p(x_{n+1}|x_n^{(k)})}{p(x_{n+1})})$, 表示的是在特定过去状态$x_n^{(k)}$下, 对未来状态$x_{n+1}$的贡献(pointwise).

主动信息存储能够捕捉系统中的复杂的, 非线性的变化. 而[自相关性](https://zh.wikipedia.org/wiki/%E8%87%AA%E7%9B%B8%E5%85%B3%E5%87%BD%E6%95%B0)只是从每个过去的值中单独提取出来的线形部分.

那么, 这个$A_X$能够捕获哪些信息呢?

- 在动态中的主动存储: 系统在动态变化时主动保留信息, 而不是被动地受环境影响. 例如, 在自驾车系统中, 系统会主动记录前方障碍物的信息, 并利用这些信息在未来做出避让决策. 而被动存储是系统被动地保存信息, 不会主动选择或分析, 这种存储不考虑是否对未来有用, 只是机械地记录, 例如一个安保摄像头会被动地记录下所有经过的人和车, 无论这些信息是否有用, 它都不会进行任何处理, 只是把信息保存下来
- 内部因果信息: 保存的是系统内部各部分之间的因果关系, 也就是记录之前的状态如何导致当前状态, 以便推导出未来的状态
- 分布式信息: 保存的是系统内部不同部分相互影响的信息, 通过网络或者反馈回路, 多个节点协作存储和共享信息, 例如, 递归神经网络
- 输入驱动存储: 系统根据外部输入的变化存储的信息

### 如何设置历史长度

$\bm{X}_n^{(k)}=\{\}X_{n-k+1}, ..., X_{n-1}, X_n$是一种$X$的过去状态的[Takens嵌入](https://blog.csdn.net/u012267725/article/details/77828974). 也可以是延迟嵌入$\bm{X}_n^{(k, \tau)}=\{X_{n-(k-1)\tau}, ..., X_{n-\tau}, X_n\}$, 即每个过去的值之间间隔$\tau$个时间点.

我们的目的是找到最佳的$k$, 使得能够最佳地预测下一个值$X_{n+1}$. 也就是说$p(X_{n+1}|\bm{x}_n^{(k)}, \bm{x}_{n-k})=p(x_{n+1}|\bm{x}_n^{(k)})$, 意思就是说已经有了过去$k$个时刻的信息$\bm{x}_n^{(k)}$, 那么进一步增加更早的时间点的信息$\bm{x}_{n-k}$不会再对预测未来的状态产生影响.

一方面, 我们希望$k$尽可能大, 以捕获所有的潜在记忆; 但是另一方面, 增加$k$会存在过度采样的风险. 理想状态下, 存在一个"甜蜜点", sweet spot. 如果我们增加历史值, 冗余的信息没有帮助, 或者由于数据不足, 无法验证过去的信息对当前的影响.

主要有三种方法设置历史长度.

#### Ragwitz准则

Ragwitz准则用于最小化系统预测的误差.

具体做法是, 对于每一个Takens嵌入向量$\bm{x}_n^{(k)}$, 在嵌入空间中寻找它的$m$个最近邻向量, 也就是与当前嵌入向量最接近的$m$个嵌入向量. 对于每个找到的近邻向量, 都对未来时刻的值$x_{n+1}$进行预测, 然后对$m$个预测结果取平均. 平均的结果和实际的$x_{n+1}$值进行比较, 计算它们之间的差异, 这就是误差. 对于所有不同长度$k$, 不同延迟$\tau$的嵌入向量走一遍上述流程, 使得误差最小化.

#### 最大化偏差矫正的AIS

这种思想其实和[统计性检验](/information-theory/statistical-significance/#significance-test)那里的思想差不多. 

公式为$A_X' =A_X-<A_X^S>$. 其中, $A_X$表示的是AIS的原始值, $A_X^S$是一种替代值, 通过打破过去和未来之间的关系生成, $<A_X^S>$是多个替代值的平均. 通过减去这些替代值的平均, 可以消除由高纬度和复杂性带来的偏差, 最大化得到的偏差矫正后的AIS. 我们可以通过在不同的历史长度和延迟组合下计算偏差矫正的AIS, 选择能够最大化$A'_X$的参数的组合.

???+ tip "Tip"

    [KSG估计器](/information-theory/estimator/#ksg). 已经内置了偏差矫正功能, 因此使用的时候无需额外矫正. 

#### 非均匀嵌入

这种方法不像前面几种方法根据固定的历史长度和延迟来选择嵌入时间点, 而是逐步选择对未来预测具有统计显著贡献的历史点.

起核心思想是在选择每个新的历史点的时候, 系统会检查它对预测未来值是否提供了额外的信息, 如果新的点带来的信息对未来预测有显著贡献, 则选择该点纳入嵌入向量. 