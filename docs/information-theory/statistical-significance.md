---
title: 信息论:统计显著性
comments: true
---

## 背景

互信息$I(X; Y)$衡量的是两个变量$X$和$Y$的依赖关系, 如果$I(X;Y)=0$, 意味着$X$和$Y$是独立的, 没有任何信息共享. 这在理论上是正确的, 但是在实际事件中, 对于经验数据来说, 即使$X$和$Y$是独立的, 我们依然可能会观测到$I(X;Y)\neq 0$. 这是因为现实中的数据可能存在噪声或者误差, 从而导致互信息的估计不等于$0$. 

所以, 在现实中, 我们要考虑两个问题:

1. 给定的$I(X;Y)$的估计值与$0$是否有显著差异? 也就是说, 我们是否可以认为观测到的互信息实际上表明了某种依赖性, 而不是由噪声引起的
2. 为了准确地估计$I(X;Y)$, 我们需要多少样本? 即, 需要多少数据点来确定互信息的真实性或者接近真实值

## A1: 统计性检验

对于第一个问题, 可以使用统计方法检验互信息. 设原假设$H_0$为$X$与$Y$独立, 备择假设为$X$与$Y$存在依赖关系. 为了检验$H_0$, 需要估计在假设$H_0$成立的时候, 采样统计量$I(X;Y)$的概率分布, 这可以通过构建替代分布实现, 具体地说, 就是构建替代分布数据$Y^s$, 使得$Y^s$和$X$独立, 但是保留$Y$的统计特性, 形成一个替代的分布, surrogate distribution.

???+ example "例子"

    假设我们正在研究学生的学习时间$X$和考试成绩$Y$之间的关系, 为了判断学习时间是否和考试成绩相关, 我们可以计算$I(X;Y)$. 原假设$H_0$为学习时间和考试成绩独立, 即$I(X;Y)=0$, 然后构建替代分布, 随机打乱考试成绩$Y$的顺序, 生成一组"替代成绩"$Y^s$, 这些成绩保留了原始的分布特性, 但是不再与原来的学习时间$X$相关. 计算打乱后的$I(X;Y^s)$, 重复生成多次替代分布. 将实际计算的互信息$I(X;Y)$和替代分布计算得到的互信息进行比较, 计算$p$值. 如果$I(X;Y)$显著大于替代分布的互信息, 就可以认为学习时间和考试成绩之间存在统计学上的显著依赖关系, 拒绝原假设.

如下图所示, 是一个构建替代分布的过程.

<figure markdown='1'>
![](https://img.ricolxwz.io/e338ef1c8544a6b116d2cfd5f9222da9.png){ loading=lazy width='300' }
</figure>

计算替代分布的互信息, 进行p检验. 

<figure markdown='1'>
![](https://img.ricolxwz.io/4ab1b34488bc3977132f2b5dbf28b213.png){ loading=lazy width='300' }
</figure>

对于条件互信息$I(X;Y|Z)$来说, 它的估计和普通互信息类似, 都需要生成替代分布. 在生成替代分布的时候, 需要通过将条件分布$p(x|y, z)$作为$p(x|z)$来进行, 这意味着, 在固定条件$Z$的情况下, 对$Y$进行重采样.

它与普通的互信息的区别是:

- 具有方向性: 在上面, 我们生成替代分布的时候, 替代的是$p(x|y, z)$, 然后将其替换为$p(x|z)$, 这个过程中我们实际上破坏了$X$和$Y$的关系, 同时保留了$X$和$Z$的关系. 这种处理方法意味着我们在测试"给定$Z$的情况下, $Y$对$X$的影响是否显著". 如果我们反过来替代$p(y|x, z)$为$p(y|z)$, 则是在测试"给定$Z$的情况下, $X$对$Y$的影响是否显著, 这时我们就有了不同的方向性, 即信息传递的方向($X\rightarrow Y$或者$Y\rightarrow X$).
- 渐进无关性: 在渐进的情况下(即大量数据样本的情形), 对$X$或者$Y$的重采样效果相同, 不受方向性影响

### 解析方法

某些情况下, 替代分布的互信息的分布经过适当变换后服从卡方分布. 卡方分布的形状是由自由度, degrees of freedom, 决定的. 在[估计器](/algorithm/estimator)章节中, 我们讲了用于预测离散, 连续随机变量信息统计量的估计器, 不同的估计器的自由度计算方法不同, 如下表所示.

<figure markdown='1'>
![](https://img.ricolxwz.io/b7065e4909ebd15cac035d26c8d8b7e1.png){ loading=lazy width='500' }
</figure>

其中, $dim(X)$的含义是$X$的纬度. $|A_X|$表示的是$A_X$可能的取值数量. 替代分布的互信息经过的转换可以表示为$2N\times I(X;Y^S)$, 或者是$2N\times I(X;Y^S|Z)$. 注意, 单位是Nats. Nats是一种用于度量信息量的单位. 类似于比特, 但它是用自然对数来计算的信息量单位, 使用Nats做单位的时候, 信息量的计算公式为$I(X)=-\ln p(X)$. 在JIDT中, 虽然计算使用的单位是Nats, 但是最终的结果会被转换为bits. 

使用解析方法的优点是不用像常规的使用极大似然估计法找到替代分布的互信息的分布的参数, 节省了很多的计算时间. 但是它只在样本量无限大的时候才完全准确, 对于有限样本, 可能会产生偏差. 并且在下列情况下会显著偏离经验值: 1. 当分析的分布是高纬度的; 2. 对于离散估计器, 其随机变量的分布严重倾斜.

## 归一化

当我们使用有限的样本数据来估计互信息$I(X;Y)$的时候, 结果可能会收到样本数量的影响, 即使$X$和$Y$是独立的, 估计的互信息也可能会大于零. 

为了消除有限样本导致的偏差, 可以使用归一化的方法, 这里的归一化是指从原始互信息$I(X;Y)$中减去替代分布的平均互信息$<I(X;Y^S)>$, 替代分布如上面👆所说, 是通过多次重采样$Y$生成的. 公式为$I^n(X;Y)=I(X;Y)-<I(X;Y^S)>$. 这样做可以去除由于样本量有限导致的偏差, 使得$I^n(X;Y)$接近于真实的互信息.

## A2: 样本量

样本量的数量取决于当前所问的问题. 

### 统计性检验

在统计性检验的时候, 样本的数量取决于变量之间的关系的强度. 如果两个变量之间的关系很强, 则在统计检验的时候需要的样本量较少, 因为这种关系即使在较少的样本中也能被检测出来. 相反, 如果两个变量之间的关系很弱. 就需要更多的样本检测出这种关系. 从整体上来说, 样本量数量的增加有助于提高检验的灵敏度. 减少因为随机噪声产生的误差.

### 避免欠采样

假设所有的状态配置都有相同的概率被采样到. 一个经验法则是样本量至少应该是状态配置数量的三倍, 状态配置数量$|A_X|\times |A_Y|$表示$X$和$Y$所有可能取值的组合数. 对于两个二元变量, 其组合数是$2\times 2=4$. 如果变量有更多的取值或者变成多向量, 状态配置数量会显著增加.
 
但是上述假设在实际情况下并不成立. 尤其是在高维数据或者变量分布不均匀的情况下, 有些状态配置可能会频繁出现, 而另一些可能是很少甚至不出现. 在高维状态中, 所有可能的状态配置数量会非常庞大, 但是实际上只有一部分状态是"典型"的, 即这些状态的采样熵接近于联合状态的真实熵, 典型集可以看作是那些在实际数据中更有可能被观测到的状态配置, 它们频繁出现, 足以对熵产生显著贡献. 为了进行准确的估计, 我们需要确保对典型集中的状态配置有足够的采样, 建议样本量至少是典型集大小的三倍. 