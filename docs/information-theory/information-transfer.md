---
title: 信息论:信息传递
comments: false
---

## 背景

回顾一下[上节课](/information-theory/information-storage), 我们正在构建一个目标变量动态的模型, 已经考虑了目标变量过去的信息(即信息存储的部分). 那么, 现在引入一个新的"源变量", 假设已经知晓了目标变量过去的信息, 从源变量过去的信息中, 有多少信息能够帮助预测目标变量下一个状态, 这是通过信息传递衡量的.

就拿心跳消息来说吧. 

<figure markdown='1'>
![](https://img.ricolxwz.io/a7f34828dfed10ebf293bb2b0cf2b104.png){ loading=lazy width='200' }
</figure>

左侧是源变量, 右侧是目标变量. 在这种情况下, 目标变量只是简单地复制源变量的消息. 源变量的消息状态的转变符合泊松分布. 将源变量记为$s$, 目标变量记为$t$, 有$t_{n+1}=s_n$.

<figure markdown='1'>
![](https://img.ricolxwz.io/dab1a772c5a6d3dc55345d9dfb1465ea.png){ loading=lazy width='200' }
</figure>

由于$\lambda_1, \lambda_2 << 0.5$, 且$\lambda_1 < \lambda_0$, 所以原变量的状态基本上是$0$.

现在, 你需要建模目标变量, 预测目标变量的下一个状态, 从两个角度出发:

- 根据过去信息做出预测: 你需要根据目标变量过去的信息和你对系统的理解, 对目标的下一个状态做出预测. 并且, 用$1$到$10$来打分记录你的确幸程度✅
- 结合源变量更新预测: 在你基于目标变量的过去状态做出初步预测后, 结合源变量的状态来更新你的预测, 同样用$1$到$10$来打分记录你的确幸程度✅

思考源变量在什么情况下对目标变量的预测最有帮助, 在哪些情况下帮助较少.

## 传递熵

传递熵, Transfer Entropy, TE, 衡量了在已经考虑了目标过程$X$的过去状态$\bm{X}_n^{(k)}=\{X_{n-k+1}, ..., X_{n-1}, X_n\}$的情况下, 源过程$Y$中的观察值$Y_n$能够在多大程度上帮助预测目标过程$X$的下一个状态$X_{n+1}$. 换句话说, TE描述的是在已知目标变量过去状态的条件下, 源变量过去的信息对预测目标变量未来状态所提供的额外帮助.

传递熵的计算公式为$T_{Y\rightarrow X}=\lim_{k\rightarrow \infty}I(Y_n;X_{n+1}|\bm{x}_n^{(k)})$. 或者考虑$k$个目标变量的历史记录$T_{Y\rightarrow X}(k)=I(Y_n;X_{n+1}|\bm{x}_n^{(k)})$. 它还可以表示为$T_{Y\rightarrow X}(k)=<\log_2\frac{p(x_{n+1}|\bm{x}_n^{(k)}, y_n)}{p(x_{n+1}|\bm{x}_n^{(k)})}>$. $t_{Y\rightarrow X}(k)=\log_2\frac{p(x_{n+1}|\bm{x}_n^{(k)}, y_n)}{p(x_{n+1}|\bm{x}_n^{(k)})}$.

那么, 如果我们不考虑目标过程$X$的过去状态$\bm{X}_n^{(k)}$, 只考虑源过程的$Y_n$呢? 即$I(Y_n;X_{n+1}|\bm{X}_n^{(k)})$和$I(Y_n;X_{n+1})$的区别在哪里. 首先, 两个都是具有方向性的(?个人看法后者是没有方向性的), 但是前者的条件性使得TE具有动态性. 而且条件化会对互信息的值产生两种不同的影响: 

- 减少冗余信息: 互信息度量的是两个随机变量之间的总体依赖性, 然而, 这种依赖性可能包含冗余信息, 例如这两个变量通过第三个变量共享的信息, 条件互信息通过引入一个条件变量来量化两个变量在给定第三个变量的情况下的依赖关系, 从而排除了由该条件变量所引入的冗余信息. 在上述的例子中, 可以看到, 目标变量其实有相当一大部分都是由自身之前的状态决定的, 因此$Y_n$提供的信息在相当大的程度上是多余, 而条件互信息更加专注于源状态$Y_n$提供的额外信息
- 增加协同效应: 尽管目标历史$\bm{X}_n^{(k)}$对$X_{n+1}$有强烈的影响, 但是源过程$Y_n$能够提供无法由目标历史解释的额外预测能力. 这表明源过程在系统中的作用是能够和目标历史协作, 产生额外的信息增益