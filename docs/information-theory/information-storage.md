---
title: 信息论:信息存储
comments: false
---

## 背景 

试着考虑如下的问题: 我们需要从历史的变量中知晓多少数据才能帮助我们做出准确的预测. 在建立时间序列等动态模型的时候, 模型中需要存储多少的过去的信息, 以便正确描述该变量未来的动态?

对于人来说, 我们可以用肉眼或者说是直觉根据过去的值判断下一个值.

<figure markdown='1'>
![](https://img.ricolxwz.io/6a17f69ad6bd7041781666e8eafdb028.png){ loading=lazy width='500' }
</figure>

如上面的图片, 需要根据已经给出的数据来预测$x(n)$的下一个值. 在这个过程中, 做了什么假设呢? 例如, 我们可能假设这些值在后续的$n$中继续保持不变, 或者呈现某种趋势/规律. 具体是从哪里获取信息来做出这个假设的呢? 例如, 我们可能会说基于所有的数据点保持不变这一信息, 做出了下一个值也不变的假设. 那么这个从过去获取信息的量是多少呢? 

## 主动信息存储

主动信息存储, Active Information Storage, 探讨的是变量的过去状态中平均存储了多少信息可以用于预测下一次观测值.

整个过去状态对未来的贡献可以被表示为$A_X = \lim_{k\rightarrow \infty}I(\bm{X}_n^{(k)};X_{n+1})$, 其中$\bm{X}_n^{(k)}=\{X_{n-k+1}, ..., X_{n-1}, X_n\}$, 表示过去的$k$个状态, $k$趋近于无穷大表示我们考虑的是整个过去的状态, 最终得到的互信息表示的就是整个过去状态对未来的贡献. 或者也可以写为$k$个过去状态对未来的贡献, 这个时候$k$就不是趋近于无穷了, 是一个值, $A_X(k)=I(\bm{X}_n^{(k)};X_{n+1})$. 

$A_X$还可以表示为$A_X=H(X_{n+1})-H_{\mu X}$, 其中, $H_{\mu X}$是[熵率](/information-theory/information-processing/#entropy-rate), 表示的是每个时间步所带来的平均熵, 或者说是单位块长度的熵增量; $H(X_{n+1})$表示的是对未来状态的全部不确定性. 这个公式可以理解为去掉预测的不确定性后, 所带来的信息增益.

$A_X(k)$可以表示在有限的历史长度$k$下, 当前状态对未来状态的预测能力的平均值, 还可以被表示为$A_X(k)=<\log_2 (\frac{p(x_{n+1}|x_n^{(k)})}{p(x_{n+1})})>$, 其中$a_X(k)=\log_2 (\frac{p(x_{n+1}|x_n^{(k)})}{p(x_{n+1})})$, 表示的是在特定过去状态$x_n^{(k)}$下, 对未来状态$x_{n+1}$的贡献(pointwise).